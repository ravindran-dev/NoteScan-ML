{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec25f6f",
   "metadata": {},
   "source": [
    "# Handwritten Text Recognition (HTR) — Notebook\n",
    "\n",
    "This notebook contains a practical pipeline for converting handwritten note images to editable text using OpenCV preprocessing and a pretrained TrOCR model (Hugging Face). Run the cells sequentially. Where long installations or downloads are required, follow the comments in the first cell.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12eb09f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/NoteScan-ML/menv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports ready\n"
     ]
    }
   ],
   "source": [
    "# 1 — Imports\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "from jiwer import wer, cer\n",
    "\n",
    "print('imports ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b84938",
   "metadata": {},
   "source": [
    "## Device Configuration\n",
    "### Description\n",
    "\n",
    "This block detects whether a GPU is available and selects the appropriate computation device. Using a GPU significantly improves inference speed for transformer-based OCR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752d7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor (slow tokenizer)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/NoteScan-ML/menv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready ✅\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"microsoft/trocr-small-handwritten\"\n",
    "\n",
    "print(\"Loading processor (slow tokenizer)...\")\n",
    "processor = TrOCRProcessor.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    MODEL_NAME\n",
    ").to(DEVICE)\n",
    "\n",
    "GEN_KWARGS = {\n",
    "    \"max_length\": 512,\n",
    "    \"num_beams\": 4,\n",
    "    \"early_stopping\": True\n",
    "}\n",
    "\n",
    "print(\"Model ready \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc021d",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "### Description\n",
    "\n",
    "Defines the pretrained TrOCR model to be used for handwritten text recognition. Generation parameters such as maximum output length and beam search settings are also initialized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe007da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing utils ready\n"
     ]
    }
   ],
   "source": [
    "# 3 — Image preprocessing utilities\n",
    "\n",
    "def show_image(img, title=None):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        if img.ndim == 2:\n",
    "            plt.imshow(img, cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "\n",
    "def preprocess_image_cv(img_pil, show=False):\n",
    "    img = np.array(img_pil.convert(\"RGB\"))\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Light denoising only\n",
    "    gray = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
    "\n",
    "    # Normalize contrast\n",
    "    gray = cv2.normalize(gray, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "    processed = Image.fromarray(gray).convert(\"RGB\")\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(processed)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    return processed\n",
    "\n",
    "print('preprocessing utils ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e786a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_lines(img_pil):\n",
    "    img = np.array(img_pil.convert(\"L\"))\n",
    "\n",
    "    # Invert for projection\n",
    "    _, th = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Horizontal projection\n",
    "    projection = np.sum(th, axis=1)\n",
    "\n",
    "    lines = []\n",
    "    start = None\n",
    "\n",
    "    for i, val in enumerate(projection):\n",
    "        if val > 0 and start is None:\n",
    "            start = i\n",
    "        elif val == 0 and start is not None:\n",
    "            end = i\n",
    "            if end - start > 15:  # minimum line height\n",
    "                lines.append((start, end))\n",
    "            start = None\n",
    "\n",
    "    if start is not None:\n",
    "        lines.append((start, len(projection)))\n",
    "\n",
    "    line_images = []\n",
    "    for (y1, y2) in lines:\n",
    "        line = img[y1:y2, :]\n",
    "        line_images.append(Image.fromarray(line).convert(\"RGB\"))\n",
    "\n",
    "    return line_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "171dde53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img_pil, model, processor, device=DEVICE):\n",
    "    if img_pil.mode != \"RGB\":\n",
    "        img_pil = img_pil.convert(\"RGB\")\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 512,\n",
    "        \"num_beams\": 4,\n",
    "        \"early_stopping\": True,\n",
    "    }\n",
    "\n",
    "    pixel_values = processor(images=img_pil, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    generated_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "    preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5ef6b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postprocessing ready\n"
     ]
    }
   ],
   "source": [
    "# 5 — Postprocessing & normalization\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace('\\r','')\n",
    "    text = re.sub(r\"\\s+\", ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "print('postprocessing ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c131745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 18:57:18,742: E symspellpy.symspellpy] Dictionary file not found at frequency_dictionary_en_82_765.txt.\n"
     ]
    }
   ],
   "source": [
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "sym_spell.load_dictionary(\n",
    "    \"frequency_dictionary_en_82_765.txt\",\n",
    "    term_index=0,\n",
    "    count_index=1\n",
    ")\n",
    "\n",
    "def correct_text(text):\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        corrected.append(suggestions[0].term if suggestions else word)\n",
    "    return \" \".join(corrected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "789e3ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FILE: test.png\n",
      "============================================================\n",
      "Circle Advocate\n",
      "' That Automatic involves state and transition among\n",
      "skilled in response to inputs. Frick Automala is a\n",
      "mathematical model of a system with discrete inputs\n",
      "and outputs. The system can be in any one of finite\n",
      "member of stores and the state summarises the\n",
      "childreny of past impetus and determines the behavior\n",
      "of the system for subsequent input.\n"
     ]
    }
   ],
   "source": [
    "# 6 — Quick demo (add images in ./data/samples/)\n",
    "SAMPLES_DIR = \"./data/samples\"\n",
    "\n",
    "if not os.path.exists(SAMPLES_DIR):\n",
    "    print(\"No sample folder found. Create\", SAMPLES_DIR, \"and add some images to run the demo.\")\n",
    "else:\n",
    "    img_paths = sorted(\n",
    "        glob.glob(os.path.join(SAMPLES_DIR, \"*.png\")) +\n",
    "        glob.glob(os.path.join(SAMPLES_DIR, \"*.jpg\")) +\n",
    "        glob.glob(os.path.join(SAMPLES_DIR, \"*.jpeg\"))\n",
    "    )\n",
    "\n",
    "    if not img_paths:\n",
    "        print(\"Sample folder is empty. Add some handwritten images.\")\n",
    "\n",
    "    for p in img_paths:\n",
    "        pil = load_image(p)\n",
    "\n",
    "        # preprocess page\n",
    "        proc = preprocess_image_cv(pil, show=False)\n",
    "\n",
    "        # segment page into lines\n",
    "        lines = segment_lines(proc)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"FILE: {os.path.basename(p)}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for line in lines:\n",
    "            # skip very small regions\n",
    "            if line.size[1] < 20:\n",
    "                continue\n",
    "\n",
    "            text = predict_image(line, model, processor)\n",
    "            text = normalize_text(text)\n",
    "            text = correct_text(text)\n",
    "\n",
    "            if len(text) > 2:\n",
    "                print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c38a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "def pdf_to_images(pdf_path, dpi=300):\n",
    "    \"\"\"\n",
    "    Converts a PDF into a list of PIL Images\n",
    "    \"\"\"\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi)\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea39061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def is_valid_text_line(line_img):\n",
    "    \"\"\"\n",
    "    Heuristic to reject non-text lines like:\n",
    "    - separators\n",
    "    - underline-only\n",
    "    - tables\n",
    "    - rows of zeros\n",
    "    \"\"\"\n",
    "    gray = np.array(line_img.convert(\"L\"))\n",
    "\n",
    "    # Reject very thin lines\n",
    "    if gray.shape[0] < 25:\n",
    "        return False\n",
    "\n",
    "    # Compute ink density\n",
    "    ink_pixels = np.sum(gray < 200)\n",
    "    total_pixels = gray.size\n",
    "    ink_ratio = ink_pixels / total_pixels\n",
    "\n",
    "    # Too little ink → separators\n",
    "    if ink_ratio < 0.01:\n",
    "        return False\n",
    "\n",
    "    # Too much ink → solid bars / tables\n",
    "    if ink_ratio > 0.6:\n",
    "        return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ede2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_pdf(pdf_path, max_pages=5):\n",
    "    pages = pdf_to_images(pdf_path)\n",
    "    full_text = []\n",
    "\n",
    "    for page_num, page in enumerate(pages[:max_pages], start=1):\n",
    "        print(f\"\\n--- Processing Page {page_num} ---\")\n",
    "\n",
    "        proc = preprocess_image_cv(page)\n",
    "        lines = segment_lines(proc)\n",
    "\n",
    "        page_lines = []\n",
    "        for line in lines:\n",
    "            if not is_valid_text_line(line):\n",
    "                continue\n",
    "\n",
    "            text = predict_image(line, model, processor)\n",
    "            text = normalize_text(text)\n",
    "            text = correct_text(text)\n",
    "\n",
    "            # reject garbage outputs\n",
    "            if len(text) < 3:\n",
    "                continue\n",
    "            if sum(c.isdigit() for c in text) / len(text) > 0.4:\n",
    "                continue\n",
    "\n",
    "            print(text)\n",
    "\n",
    "            \n",
    "\n",
    "            if len(text) > 2:\n",
    "                page_lines.append(text)\n",
    "\n",
    "        full_text.append(\"\\n\".join(page_lines))\n",
    "\n",
    "    return \"\\n\\n\".join(full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2901f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Page 1 ---\n",
      "1 Mechanisms _____________________________________\n",
      "#______\n",
      "1 References\n",
      "implied string is acceptable or not.\n",
      "A finite automation has a mechanism to\n",
      "it will\n",
      "I read impact, which is a strong over a given\n",
      "\", alphabetical. This impul is actually written on an\n",
      "if fundamental strategies are used as an increase in\n",
      "\n",
      "--- Processing Page 2 ---\n",
      "1 Mechanisms _____________________________________\n",
      "#______\n",
      "a b c\n",
      "displaystyle _ 0\n",
      "2 Legal files\n",
      "a man and wait\n",
      "References\n",
      "\n",
      "--- Processing Page 3 ---\n",
      "1 Mechanisms _____________________________________\n",
      "#______\n",
      "# Other qualifications of the IFPI accepts the\n",
      "\" Language! of strings that have a subslying\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdf_path = \"./data/samples/sample.pdf\"\n",
    "final_text = ocr_pdf(pdf_path, max_pages=5)\n",
    "\n",
    "print(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d734eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def export_to_docx(text, output_path):\n",
    "    doc = Document()\n",
    "    for line in text.split(\"\\n\"):\n",
    "        doc.add_paragraph(line)\n",
    "    doc.save(output_path)\n",
    "\n",
    "export_to_docx(final_text, \"output_first_5_pages.docx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f0377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ evaluation helper ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_predictions(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    preds = []\n",
    "    gts = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        img = load_image(row[\"image_path\"])\n",
    "        proc = preprocess_image_cv(img)\n",
    "\n",
    "        # segment page into lines\n",
    "        lines = segment_lines(proc)\n",
    "\n",
    "        # match GT line index\n",
    "        gt_text = normalize_text(row[\"transcription\"])\n",
    "\n",
    "        # find best matching predicted line\n",
    "        line_preds = []\n",
    "        for line in lines:\n",
    "            text = predict_image(line, model, processor)\n",
    "            text = normalize_text(text)\n",
    "            if len(text) > 2:\n",
    "                line_preds.append(text)\n",
    "\n",
    "        if not line_preds:\n",
    "            continue\n",
    "\n",
    "        # choose longest predicted line (simple heuristic)\n",
    "        pred_text = max(line_preds, key=len)\n",
    "\n",
    "        preds.append(pred_text)\n",
    "        gts.append(gt_text)\n",
    "\n",
    "    return {\n",
    "        \"CER\": cer(gts, preds),\n",
    "        \"WER\": wer(gts, preds),\n",
    "        \"samples\": len(gts)\n",
    "    }\n",
    "\n",
    "print(\"✅ evaluation helper ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facfeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export functions ready\n"
     ]
    }
   ],
   "source": [
    "# 8 — Export functions (DOCX / PDF)\n",
    "from docx import Document\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def save_as_docx(text, out_path):\n",
    "    doc = Document()\n",
    "    for line in text.split('\\n'):\n",
    "        doc.add_paragraph(line)\n",
    "    doc.save(out_path)\n",
    "\n",
    "def save_as_pdf(text, out_path):\n",
    "    c = canvas.Canvas(out_path, pagesize=letter)\n",
    "    width, height = letter\n",
    "    y = height - 72\n",
    "    for line in text.split('\\n'):\n",
    "        c.drawString(72, y, line)\n",
    "        y -= 14\n",
    "        if y < 72:\n",
    "            c.showPage()\n",
    "            y = height - 72\n",
    "    c.save()\n",
    "\n",
    "print('export functions ready')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
