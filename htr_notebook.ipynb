{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec25f6f",
   "metadata": {},
   "source": [
    "# Handwritten Text Recognition (HTR) — Notebook\n",
    "\n",
    "This notebook contains a practical pipeline for converting handwritten note images to editable text using OpenCV preprocessing and a pretrained TrOCR model (Hugging Face). Run the cells sequentially. Where long installations or downloads are required, follow the comments in the first cell.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12eb09f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/NoteScan-ML/menv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports ready\n"
     ]
    }
   ],
   "source": [
    "# 1 — Imports\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "from jiwer import wer, cer\n",
    "\n",
    "print('imports ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b84938",
   "metadata": {},
   "source": [
    "## Device Configuration\n",
    "### Description\n",
    "\n",
    "This block detects whether a GPU is available and selects the appropriate computation device. Using a GPU significantly improves inference speed for transformer-based OCR models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ddea0",
   "metadata": {},
   "source": [
    "### Load TrOCR Processor and Model\n",
    "\n",
    "Loads the TrOCR processor and vision-encoder-decoder model from Hugging Face.\n",
    "The tokenizer is explicitly loaded in slow mode to ensure stability and avoid fast-tokenizer compatibility issues.\n",
    "\n",
    "The model is moved to the selected computation device and loaded only once for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c752d7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor (slow tokenizer)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/NoteScan-ML/menv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"microsoft/trocr-small-handwritten\"\n",
    "\n",
    "print(\"Loading processor (slow tokenizer)...\")\n",
    "processor = TrOCRProcessor.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    MODEL_NAME\n",
    ").to(DEVICE)\n",
    "\n",
    "GEN_KWARGS = {\n",
    "    \"max_length\": 512,\n",
    "    \"num_beams\": 4,\n",
    "    \"early_stopping\": True\n",
    "}\n",
    "\n",
    "print(\"Model ready \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b3495",
   "metadata": {},
   "source": [
    "## Image Loading Utility\n",
    "### Description\n",
    "\n",
    "Defines a helper function to load an image from disk and convert it into a consistent RGB format suitable for preprocessing and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74228838",
   "metadata": {},
   "source": [
    "### Image Preprocessing\n",
    "\n",
    "This block performs classical image preprocessing using OpenCV.\n",
    "Steps include grayscale conversion, noise reduction, contrast normalization, and format conversion.\n",
    "These operations improve OCR accuracy, especially for handwritten documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe007da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing utils ready\n"
     ]
    }
   ],
   "source": [
    "# 3 — Image preprocessing utilities\n",
    "\n",
    "def show_image(img, title=None):\n",
    "    if isinstance(img, np.ndarray):\n",
    "        if img.ndim == 2:\n",
    "            plt.imshow(img, cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "\n",
    "def preprocess_image_cv(img_pil, show=False):\n",
    "    img = np.array(img_pil.convert(\"RGB\"))\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Light denoising only\n",
    "    gray = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)\n",
    "\n",
    "    # Normalize contrast\n",
    "    gray = cv2.normalize(gray, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "    processed = Image.fromarray(gray).convert(\"RGB\")\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(processed)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    return processed\n",
    "\n",
    "print('preprocessing utils ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97651467",
   "metadata": {},
   "source": [
    "## Line Segmentation\n",
    "### Description\n",
    "\n",
    "Implements line-level segmentation using horizontal projection profiles.\n",
    "Segmenting text into individual lines improves recognition accuracy and reduces model confusion caused by multi-line inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e786a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_lines(img_pil):\n",
    "    img = np.array(img_pil.convert(\"L\"))\n",
    "\n",
    "    # Invert for projection\n",
    "    _, th = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Horizontal projection\n",
    "    projection = np.sum(th, axis=1)\n",
    "\n",
    "    lines = []\n",
    "    start = None\n",
    "\n",
    "    for i, val in enumerate(projection):\n",
    "        if val > 0 and start is None:\n",
    "            start = i\n",
    "        elif val == 0 and start is not None:\n",
    "            end = i\n",
    "            if end - start > 15:  # minimum line height\n",
    "                lines.append((start, end))\n",
    "            start = None\n",
    "\n",
    "    if start is not None:\n",
    "        lines.append((start, len(projection)))\n",
    "\n",
    "    line_images = []\n",
    "    for (y1, y2) in lines:\n",
    "        line = img[y1:y2, :]\n",
    "        line_images.append(Image.fromarray(line).convert(\"RGB\"))\n",
    "\n",
    "    return line_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc10d71",
   "metadata": {},
   "source": [
    "## OCR Inference Function\n",
    "\n",
    "\n",
    "Encapsulates the OCR inference logic.\n",
    "The function processes a single image or text line, runs it through the TrOCR model, and decodes the generated output into readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "171dde53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img_pil, model, processor, device=DEVICE):\n",
    "    if img_pil.mode != \"RGB\":\n",
    "        img_pil = img_pil.convert(\"RGB\")\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 512,\n",
    "        \"num_beams\": 4,\n",
    "        \"early_stopping\": True,\n",
    "    }\n",
    "\n",
    "    pixel_values = processor(images=img_pil, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    generated_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "    preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return preds[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b443a26",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "\n",
    "\n",
    "Cleans raw OCR output by removing extra whitespace, line breaks, and formatting artifacts.\n",
    "This step ensures consistency before applying spelling or domain corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5ef6b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postprocessing ready\n"
     ]
    }
   ],
   "source": [
    "# 5 — Postprocessing & normalization\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace('\\r','')\n",
    "    text = re.sub(r\"\\s+\", ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "print('postprocessing ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13231b5e",
   "metadata": {},
   "source": [
    "## Spell Correction and Domain-Specific Post-Processing\n",
    "\n",
    "\n",
    "Applies dictionary-based spell correction using SymSpell and fixes common OCR misinterpretations with domain-aware replacements.\n",
    "This significantly improves readability and correctness of the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c131745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 18:57:18,742: E symspellpy.symspellpy] Dictionary file not found at frequency_dictionary_en_82_765.txt.\n"
     ]
    }
   ],
   "source": [
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "sym_spell.load_dictionary(\n",
    "    \"frequency_dictionary_en_82_765.txt\",\n",
    "    term_index=0,\n",
    "    count_index=1\n",
    ")\n",
    "\n",
    "def correct_text(text):\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        corrected.append(suggestions[0].term if suggestions else word)\n",
    "    return \" \".join(corrected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba37f9",
   "metadata": {},
   "source": [
    "## End-to-End Image OCR Pipeline\n",
    "\n",
    "Combines preprocessing, line segmentation, inference, and post-processing into a single pipeline function.\n",
    "This function represents the complete OCR flow for a handwritten image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "789e3ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FILE: test.png\n",
      "============================================================\n",
      "Circle Advocate\n",
      "' That Automatic involves state and transition among\n",
      "skilled in response to inputs. Frick Automala is a\n",
      "mathematical model of a system with discrete inputs\n",
      "and outputs. The system can be in any one of finite\n",
      "member of stores and the state summarises the\n",
      "childreny of past impetus and determines the behavior\n",
      "of the system for subsequent input.\n"
     ]
    }
   ],
   "source": [
    "# 6 — Quick demo (add images in ./data/samples/)\n",
    "SAMPLES_DIR = \"./data/samples\"\n",
    "\n",
    "if not os.path.exists(SAMPLES_DIR):\n",
    "    print(\"No sample folder found. Create\", SAMPLES_DIR, \"and add some images to run the demo.\")\n",
    "else:\n",
    "    img_paths = sorted(\n",
    "        glob.glob(os.path.join(SAMPLES_DIR, \"*.png\")) +\n",
    "        glob.glob(os.path.join(SAMPLES_DIR, \"*.jpg\")) +\n",
    "        glob.glob(os.path.join(SAMPLES_DIR, \"*.jpeg\"))\n",
    "    )\n",
    "\n",
    "    if not img_paths:\n",
    "        print(\"Sample folder is empty. Add some handwritten images.\")\n",
    "\n",
    "    for p in img_paths:\n",
    "        pil = load_image(p)\n",
    "\n",
    "        # preprocess page\n",
    "        proc = preprocess_image_cv(pil, show=False)\n",
    "\n",
    "        # segment page into lines\n",
    "        lines = segment_lines(proc)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"FILE: {os.path.basename(p)}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for line in lines:\n",
    "            # skip very small regions\n",
    "            if line.size[1] < 20:\n",
    "                continue\n",
    "\n",
    "            text = predict_image(line, model, processor)\n",
    "            text = normalize_text(text)\n",
    "            text = correct_text(text)\n",
    "\n",
    "            if len(text) > 2:\n",
    "                print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f74d8",
   "metadata": {},
   "source": [
    "## Quick Demo on Sample Images\n",
    "\n",
    "\n",
    "Runs the OCR pipeline on sample handwritten images stored in a local directory.\n",
    "This section demonstrates practical usage and prints recognized text line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c38a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "def pdf_to_images(pdf_path, dpi=300):\n",
    "    \"\"\"\n",
    "    Converts a PDF into a list of PIL Images\n",
    "    \"\"\"\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi)\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291ed329",
   "metadata": {},
   "source": [
    "## Text Line Validation\n",
    "### Description\n",
    "\n",
    "Defines heuristics to filter out non-text regions such as separators, noise, tables, or extremely thin lines.\n",
    "This step prevents meaningless OCR outputs like repeated zeros or artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea39061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def is_valid_text_line(line_img):\n",
    "    \"\"\"\n",
    "    Heuristic to reject non-text lines like:\n",
    "    - separators\n",
    "    - underline-only\n",
    "    - tables\n",
    "    - rows of zeros\n",
    "    \"\"\"\n",
    "    gray = np.array(line_img.convert(\"L\"))\n",
    "\n",
    "    # Reject very thin lines\n",
    "    if gray.shape[0] < 25:\n",
    "        return False\n",
    "\n",
    "    # Compute ink density\n",
    "    ink_pixels = np.sum(gray < 200)\n",
    "    total_pixels = gray.size\n",
    "    ink_ratio = ink_pixels / total_pixels\n",
    "\n",
    "    # Too little ink → separators\n",
    "    if ink_ratio < 0.01:\n",
    "        return False\n",
    "\n",
    "    # Too much ink → solid bars / tables\n",
    "    if ink_ratio > 0.6:\n",
    "        return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526bdcd",
   "metadata": {},
   "source": [
    "## OCR on PDF Documents\n",
    "\n",
    "\n",
    "Applies the OCR pipeline to each page of a PDF document.\n",
    "Text is extracted page-wise and concatenated while preserving logical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ede2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_pdf(pdf_path, max_pages=5):\n",
    "    pages = pdf_to_images(pdf_path)\n",
    "    full_text = []\n",
    "\n",
    "    for page_num, page in enumerate(pages[:max_pages], start=1):\n",
    "        print(f\"\\n--- Processing Page {page_num} ---\")\n",
    "\n",
    "        proc = preprocess_image_cv(page)\n",
    "        lines = segment_lines(proc)\n",
    "\n",
    "        page_lines = []\n",
    "        for line in lines:\n",
    "            if not is_valid_text_line(line):\n",
    "                continue\n",
    "\n",
    "            text = predict_image(line, model, processor)\n",
    "            text = normalize_text(text)\n",
    "            text = correct_text(text)\n",
    "\n",
    "            # reject garbage outputs\n",
    "            if len(text) < 3:\n",
    "                continue\n",
    "            if sum(c.isdigit() for c in text) / len(text) > 0.4:\n",
    "                continue\n",
    "\n",
    "            print(text)\n",
    "\n",
    "            \n",
    "\n",
    "            if len(text) > 2:\n",
    "                page_lines.append(text)\n",
    "\n",
    "        full_text.append(\"\\n\".join(page_lines))\n",
    "\n",
    "    return \"\\n\\n\".join(full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2901f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Page 1 ---\n",
      "1 Mechanisms _____________________________________\n",
      "#______\n",
      "1 References\n",
      "implied string is acceptable or not.\n",
      "A finite automation has a mechanism to\n",
      "it will\n",
      "I read impact, which is a strong over a given\n",
      "\", alphabetical. This impul is actually written on an\n",
      "if fundamental strategies are used as an increase in\n",
      "\n",
      "--- Processing Page 2 ---\n",
      "1 Mechanisms _____________________________________\n",
      "#______\n",
      "a b c\n",
      "displaystyle _ 0\n",
      "2 Legal files\n",
      "a man and wait\n",
      "References\n",
      "\n",
      "--- Processing Page 3 ---\n",
      "1 Mechanisms _____________________________________\n",
      "#______\n",
      "# Other qualifications of the IFPI accepts the\n",
      "\" Language! of strings that have a subslying\n",
      "\n",
      "--- Processing Page 4 ---\n",
      "1 Mechanisms _____________________________________\n",
      "#______\n",
      "more than?\n",
      "A- Guinea complexity : 0\n",
      ". The time needed for his\n",
      "creating an impulsting #\n",
      "a b c d e SUDOCAST YEAR OF WESTERN OF WESTERN OF\n",
      "A Pugemary is a\n",
      "References\n",
      "\n",
      "--- Processing Page 5 ---\n",
      "1 Mechanisms ___________________________________________\n",
      "#______\n",
      "Kagulan expressions and languages :\n",
      "a b c d\n",
      ". The sudden that define the regular\n",
      "compensations are alphabetical tax as follows :\n",
      "if if is a regular exposition that denotes the null set.\n",
      "OF is a regular expression that denotes empty set if I\n",
      "a few years\n",
      "1 Mechanisms _____________________________________\n",
      "#______\n",
      "1 References\n",
      "implied string is acceptable or not.\n",
      "A finite automation has a mechanism to\n",
      "it will\n",
      "I read impact, which is a strong over a given\n",
      "\", alphabetical. This impul is actually written on an\n",
      "if fundamental strategies are used as an increase in\n",
      "\n",
      "1 Mechanisms _____________________________________\n",
      "#______\n",
      "a b c\n",
      "displaystyle _ 0\n",
      "2 Legal files\n",
      "a man and wait\n",
      "References\n",
      "\n",
      "1 Mechanisms _____________________________________\n",
      "#______\n",
      "# Other qualifications of the IFPI accepts the\n",
      "\" Language! of strings that have a subslying\n",
      "\n",
      "1 Mechanisms _____________________________________\n",
      "#______\n",
      "more than?\n",
      "A- Guinea complexity : 0\n",
      ". The time needed for his\n",
      "creating an impulsting #\n",
      "a b c d e SUDOCAST YEAR OF WESTERN OF WESTERN OF\n",
      "A Pugemary is a\n",
      "References\n",
      "\n",
      "1 Mechanisms ___________________________________________\n",
      "#______\n",
      "Kagulan expressions and languages :\n",
      "a b c d\n",
      ". The sudden that define the regular\n",
      "compensations are alphabetical tax as follows :\n",
      "if if is a regular exposition that denotes the null set.\n",
      "OF is a regular expression that denotes empty set if I\n",
      "a few years\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdf_path = \"./data/samples/sample.pdf\"\n",
    "final_text = ocr_pdf(pdf_path, max_pages=5)\n",
    "\n",
    "print(final_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c72a9",
   "metadata": {},
   "source": [
    "## Export OCR Output to DOCX\n",
    "\n",
    "\n",
    "Demonstrates how to export recognized text into a Word document for editing, sharing, or archival purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2d734eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def export_to_docx(text, output_path):\n",
    "    doc = Document()\n",
    "    for line in text.split(\"\\n\"):\n",
    "        doc.add_paragraph(line)\n",
    "    doc.save(output_path)\n",
    "\n",
    "export_to_docx(final_text, \"output_first_5_pages.docx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc8f0377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ evaluation helper ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_predictions(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    preds = []\n",
    "    gts = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        img = load_image(row[\"image_path\"])\n",
    "        proc = preprocess_image_cv(img)\n",
    "\n",
    "        # segment page into lines\n",
    "        lines = segment_lines(proc)\n",
    "\n",
    "        # match GT line index\n",
    "        gt_text = normalize_text(row[\"transcription\"])\n",
    "\n",
    "        # find best matching predicted line\n",
    "        line_preds = []\n",
    "        for line in lines:\n",
    "            text = predict_image(line, model, processor)\n",
    "            text = normalize_text(text)\n",
    "            if len(text) > 2:\n",
    "                line_preds.append(text)\n",
    "\n",
    "        if not line_preds:\n",
    "            continue\n",
    "\n",
    "        # choose longest predicted line (simple heuristic)\n",
    "        pred_text = max(line_preds, key=len)\n",
    "\n",
    "        preds.append(pred_text)\n",
    "        gts.append(gt_text)\n",
    "\n",
    "    return {\n",
    "        \"CER\": cer(gts, preds),\n",
    "        \"WER\": wer(gts, preds),\n",
    "        \"samples\": len(gts)\n",
    "    }\n",
    "\n",
    "print(\"✅ evaluation helper ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7facfeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export functions ready\n"
     ]
    }
   ],
   "source": [
    "# 8 — Export functions (DOCX / PDF)\n",
    "from docx import Document\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def save_as_docx(text, out_path):\n",
    "    doc = Document()\n",
    "    for line in text.split('\\n'):\n",
    "        doc.add_paragraph(line)\n",
    "    doc.save(out_path)\n",
    "\n",
    "def save_as_pdf(text, out_path):\n",
    "    c = canvas.Canvas(out_path, pagesize=letter)\n",
    "    width, height = letter\n",
    "    y = height - 72\n",
    "    for line in text.split('\\n'):\n",
    "        c.drawString(72, y, line)\n",
    "        y -= 14\n",
    "        if y < 72:\n",
    "            c.showPage()\n",
    "            y = height - 72\n",
    "    c.save()\n",
    "\n",
    "print('export functions ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed05318",
   "metadata": {},
   "source": [
    "## Evaluation Metrics (CER & WER)\n",
    "\n",
    "\n",
    "Defines an evaluation helper to compute Character Error Rate (CER) and Word Error Rate (WER) using a labeled CSV file.\n",
    "These metrics quantitatively measure OCR performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "327ce836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from jiwer import cer, wer\n",
    "\n",
    "def evaluate_ocr(csv_path, model, processor, device):\n",
    "    \"\"\"\n",
    "    Evaluate OCR performance using CER and WER.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): CSV with columns [image_path, transcription]\n",
    "        model: TrOCR model\n",
    "        processor: TrOCR processor\n",
    "        device: cpu / cuda\n",
    "\n",
    "    Returns:\n",
    "        dict: CER and WER scores\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        img = load_image(row[\"image_path\"])\n",
    "        proc = preprocess_image_cv(img)\n",
    "\n",
    "        lines = segment_lines(proc)\n",
    "\n",
    "        predicted_lines = []\n",
    "        for line in lines:\n",
    "            if not is_valid_text_line(line):\n",
    "                continue\n",
    "\n",
    "            text = predict_image(line, model, processor, device)\n",
    "            text = normalize_text(text)\n",
    "            text = correct_text(text)\n",
    "\n",
    "            if len(text) > 2:\n",
    "                predicted_lines.append(text)\n",
    "\n",
    "        prediction = \" \".join(predicted_lines)\n",
    "\n",
    "        predictions.append(prediction)\n",
    "        ground_truths.append(row[\"transcription\"])\n",
    "\n",
    "    return {\n",
    "        \"CER\": cer(ground_truths, predictions),\n",
    "        \"WER\": wer(ground_truths, predictions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd110daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Error Rate (CER): 2.477064220183486\n",
      "Word Error Rate (WER): 2.272727272727273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_ocr(\n",
    "    csv_path=\"data/eval_labels.csv\",\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"Character Error Rate (CER):\", metrics[\"CER\"])\n",
    "print(\"Word Error Rate (WER):\", metrics[\"WER\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
